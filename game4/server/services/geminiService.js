import { GoogleGenerativeAI } from '@google/generative-ai'

// Initialize Gemini AI with API key
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY)
const model = genAI.getGenerativeModel({ model: 'gemini-pro' })

// In-memory cache for AI summaries with 1-hour TTL
const summaryCache = new Map()
const CACHE_TTL = 60 * 60 * 1000 // 1 hour in milliseconds

// Rate limiting - max 60 requests per minute (Gemini free tier limit)
let requestQueue = []
let processingQueue = false
const MAX_REQUESTS_PER_MINUTE = 60
const MINUTE_MS = 60 * 1000

/**
 * Category-specific prompts for better summarization
 */
const CATEGORY_PROMPTS = {
  stock: `ë‹¤ìŒ ì£¼ì‹/ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ìˆ˜ì¹˜, ê¸°ì—…ëª…, ë³€ë™ ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ íˆ¬ììê°€ ì•Œì•„ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë‹´ì•„ì£¼ì„¸ìš”:

{content}

ìš”ì•½:`,

  sports: `ë‹¤ìŒ ìŠ¤í¬ì¸  ë‰´ìŠ¤ë¥¼ 200ì ì´ë‚´ë¡œ ìƒë™ê° ìˆê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ê²½ê¸° ê²°ê³¼, ì„ ìˆ˜ í™œì•½, ê¸°ë¡ ë“± ìŠ¤í¬ì¸  íŒ¬ë“¤ì´ ê´€ì‹¬ ìˆì–´í•  í•˜ì´ë¼ì´íŠ¸ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:

{content}

ìš”ì•½:`,

  entertainment: `ë‹¤ìŒ ì—°ì˜ˆ/ì—”í„°í…Œì¸ë¨¼íŠ¸ ë‰´ìŠ¤ë¥¼ 200ì ì´ë‚´ë¡œ í¥ë¯¸ë¡­ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì¶œì—°ì§„, ì‘í’ˆëª…, í™”ì œì„± ë“± ëŒ€ì¤‘ì´ ê´€ì‹¬ ê°€ì§ˆ í¬ì¸íŠ¸ë¥¼ ë‹´ì•„ì£¼ì„¸ìš”:

{content}

ìš”ì•½:`,

  weather: `ë‹¤ìŒ ë‚ ì”¨ ë‰´ìŠ¤ë¥¼ 200ì ì´ë‚´ë¡œ ì‹¤ìš©ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ê¸°ì˜¨, ê°•ìˆ˜ëŸ‰, ì£¼ì˜ì‚¬í•­ ë“± ì¼ìƒìƒí™œì— ë„ì›€ë˜ëŠ” ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:

{content}

ìš”ì•½:`,

  default: `ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ 200ì ì´ë‚´ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ë…ìê°€ ë¹ ë¥´ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¤‘ìš”í•œ ë‚´ìš©ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”:

{content}

ìš”ì•½:`
}

/**
 * Rate limiting queue processor
 */
const processQueue = async () => {
  if (processingQueue || requestQueue.length === 0) return

  processingQueue = true

  while (requestQueue.length > 0) {
    const batchSize = Math.min(requestQueue.length, MAX_REQUESTS_PER_MINUTE)
    const batch = requestQueue.splice(0, batchSize)

    console.log(`ğŸ¤– Processing ${batch.length} Gemini requests`)

    // Process batch in parallel
    await Promise.all(batch.map(async (request) => {
      try {
        const result = await model.generateContent(request.prompt)
        const response = await result.response
        const summary = response.text().trim()

        // Cache the result
        summaryCache.set(request.cacheKey, {
          summary,
          timestamp: Date.now()
        })

        console.log(`âœ¨ Gemini summary generated for: ${request.title?.substring(0, 50)}...`)
        request.resolve(summary)

      } catch (error) {
        console.error(`âŒ Gemini summarization error for "${request.title}":`, error.message)

        // Fallback to original content if AI fails
        const fallbackSummary = request.originalContent.length > 200
          ? request.originalContent.substring(0, 200) + '...'
          : request.originalContent || 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'

        request.resolve(fallbackSummary)
      }
    }))

    // If there are more requests, wait a minute before processing next batch
    if (requestQueue.length > 0) {
      console.log(`â³ Rate limiting: waiting 60 seconds before processing ${requestQueue.length} more requests...`)
      await new Promise(resolve => setTimeout(resolve, MINUTE_MS))
    }
  }

  processingQueue = false
}

/**
 * Generate AI summary for news article with rate limiting
 * @param {string} content - Original article content
 * @param {string} category - Article category for context
 * @param {string} title - Article title for logging
 * @returns {Promise<string>} - AI-generated summary
 */
export const generateAISummary = async (content, category = 'default', title = '') => {
  try {
    // Check cache first
    const cacheKey = `summary_${Buffer.from(content).toString('base64').substring(0, 32)}`
    const cached = summaryCache.get(cacheKey)

    if (cached && (Date.now() - cached.timestamp) < CACHE_TTL) {
      console.log(`ğŸ’¾ Returning cached summary for: ${title?.substring(0, 50)}...`)
      return cached.summary
    }

    // If content is too short, return as is
    if (!content || content.length < 50) {
      const fallback = content || 'ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.'
      return fallback.length > 200 ? fallback.substring(0, 200) + '...' : fallback
    }

    // Create summarization prompt
    const promptTemplate = CATEGORY_PROMPTS[category] || CATEGORY_PROMPTS.default
    const prompt = promptTemplate.replace('{content}', content.substring(0, 2000)) // Limit input length

    // Add to queue and return promise
    return new Promise((resolve) => {
      requestQueue.push({
        prompt,
        cacheKey,
        originalContent: content,
        title,
        resolve
      })

      // Start processing queue
      processQueue()
    })

  } catch (error) {
    console.error('âŒ Error in generateAISummary:', error.message)

    // Return fallback summary
    const fallbackSummary = content.length > 200
      ? content.substring(0, 200) + '...'
      : content || 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'

    return fallbackSummary
  }
}

/**
 * Batch summarize multiple articles with intelligent batching
 * @param {Array} articles - Array of articles to summarize
 * @returns {Promise<Array>} - Articles with AI-generated summaries
 */
export const batchSummarizeArticles = async (articles) => {
  if (!articles || articles.length === 0) {
    return []
  }

  try {
    console.log(`ğŸš€ Starting batch summarization for ${articles.length} articles`)

    // Filter articles that need summarization (not cached and have content)
    const articlesToSummarize = []
    const results = []

    for (const article of articles) {
      const cacheKey = `summary_${Buffer.from(article.rawContent || article.summary || '').toString('base64').substring(0, 32)}`
      const cached = summaryCache.get(cacheKey)

      if (cached && (Date.now() - cached.timestamp) < CACHE_TTL) {
        // Use cached summary
        results.push({
          ...article,
          summary: cached.summary,
          aiSummarized: true
        })
      } else if (article.rawContent && article.rawContent.length > 50) {
        // Needs AI summarization
        articlesToSummarize.push(article)
      } else {
        // Use original summary/content
        results.push({
          ...article,
          summary: article.summary || article.rawContent || 'ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.',
          aiSummarized: false
        })
      }
    }

    console.log(`ğŸ“ ${articlesToSummarize.length} articles need AI summarization, ${results.length} from cache`)

    // Generate AI summaries for remaining articles
    if (articlesToSummarize.length > 0) {
      const summaryPromises = articlesToSummarize.map(async (article) => {
        const aiSummary = await generateAISummary(
          article.rawContent,
          article.category,
          article.title
        )

        return {
          ...article,
          summary: aiSummary,
          aiSummarized: true
        }
      })

      const summarizedArticles = await Promise.all(summaryPromises)
      results.push(...summarizedArticles)
    }

    // Sort by original order and return
    const sortedResults = results.sort((a, b) => {
      const indexA = articles.findIndex(article => article.id === a.id)
      const indexB = articles.findIndex(article => article.id === b.id)
      return indexA - indexB
    })

    console.log(`âœ… Batch summarization completed: ${sortedResults.filter(a => a.aiSummarized).length}/${sortedResults.length} AI-summarized`)
    return sortedResults

  } catch (error) {
    console.error('âŒ Error in batch summarization:', error.message)

    // Return articles with fallback summaries
    return articles.map(article => ({
      ...article,
      summary: article.rawContent || article.summary || 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
      aiSummarized: false
    }))
  }
}

/**
 * Get Gemini service status and statistics
 * @returns {Object} - Service status information
 */
export const getGeminiStatus = () => {
  return {
    available: !!process.env.GEMINI_API_KEY,
    cacheSize: summaryCache.size,
    queueSize: requestQueue.length,
    isProcessing: processingQueue,
    rateLimit: `${MAX_REQUESTS_PER_MINUTE}/minute`,
    cacheHitRate: summaryCache.size > 0 ? 'ê³„ì‚° ì¤‘...' : '0%'
  }
}

/**
 * Clear summary cache (useful for testing)
 */
export const clearSummaryCache = () => {
  summaryCache.clear()
  console.log('ğŸ§¹ Summary cache cleared')
}

/**
 * Health check for Gemini service
 * @returns {Promise<boolean>} - Service health status
 */
export const healthCheck = async () => {
  try {
    if (!process.env.GEMINI_API_KEY) {
      console.warn('âš ï¸ GEMINI_API_KEY not found')
      return false
    }

    // Test with a simple prompt
    const testResult = await model.generateContent('ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ "ì•ˆë…•í•˜ì„¸ìš”"ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.')
    const response = await testResult.response
    const text = response.text()

    console.log('âœ… Gemini health check passed:', text.substring(0, 50))
    return true

  } catch (error) {
    console.error('âŒ Gemini health check failed:', error.message)
    return false
  }
}